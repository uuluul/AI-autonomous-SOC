import os
import time
import json
import logging
import glob
import pika
import requests
from datetime import datetime, timedelta
from llm_client import LLMClient
from to_stix import build_stix_bundle
from to_pdf import generate_pdf_report
from extract_schema import DEFAULT_SYSTEM_PROMPT, EXTRACTION_SCHEMA_DESCRIPTION
from validate_stix import validate_stix_json
from utils import chunk_text, merge_extractions
from database import insert_task
from pii_masker import PIIMasker
from src.setup_opensearch import upsert_indicator, upload_to_opensearch as upload_to_os_lib, get_opensearch_client
from src.enrichment import EnrichmentEngine
from src.cve_enrichment import CVEEnricher
import schedule
import threading
import subprocess

logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒè®Šæ•¸
RABBITMQ_HOST = os.getenv("RABBITMQ_HOST", "rabbitmq")
RABBITMQ_QUEUE = "cti_tasks"
ROLE = os.getenv("ROLE", "master")  # 'master' or 'worker'

# è³‡æ–™å¤¾è¨­å®š
INPUT_DIR = "data/input"
PROCESSED_DIR = "data/processed"
OUTPUT_DIR = "out"

# OpenSearch è¨­å®š (ç”¨æ–¼ RAG æª¢ç´¢)
OS_HOST = os.getenv("OPENSEARCH_HOST", "opensearch-node")
OS_PORT = os.getenv("OPENSEARCH_PORT", "9200")
OS_USER = os.getenv("OPENSEARCH_USER", "admin")
OS_PASS = os.getenv("OPENSEARCH_PASSWORD", "admin")
OS_AUTH = (OS_USER, OS_PASS)
BASE_URL = f"http://{OS_HOST}:{OS_PORT}"
INDEX_KB = "cti-reports"

def ensure_dirs():
    """ç¢ºä¿å¿…è¦çš„è³‡æ–™å¤¾å­˜åœ¨"""
    for d in [INPUT_DIR, PROCESSED_DIR, OUTPUT_DIR]:
        os.makedirs(d, exist_ok=True)

# --- RabbitMQ é€£ç·š Helper ---
def get_rabbitmq_connection():
    """å»ºç«‹ RabbitMQ é€£ç·šèˆ‡ Channel"""
    credentials = pika.PlainCredentials('user', 'password')
    parameters = pika.ConnectionParameters(RABBITMQ_HOST, 5672, '/', credentials, heartbeat=600)
    
    while True:
        try:
            connection = pika.BlockingConnection(parameters)
            channel = connection.channel()
            channel.queue_declare(queue=RABBITMQ_QUEUE, durable=True)
            return connection, channel
        except pika.exceptions.AMQPConnectionError:
            logger.warning("  Waiting for RabbitMQ...")
            time.sleep(5)

# --- RAG æª¢ç´¢å‡½å¼ ---
def retrieve_context(text_content: str, top_k: int = 3) -> str:
    """
    RAG æ ¸å¿ƒï¼šåŒæ™‚æŸ¥è©¢ã€Œå¤–éƒ¨å°ˆå®¶çŸ¥è­˜ (MITRE/AI Defense)ã€èˆ‡ã€Œå…§éƒ¨æ­·å²æ¡ˆä¾‹ (Reports)ã€
    """
    if not text_content: 
        return ""
    
    client = get_opensearch_client()
    context_parts = []
    
    # æœå°‹å…§å®¹æˆªæ–·ï¼Œé¿å… Token éé•·ï¼Œä½†ä¿ç•™è¶³å¤ èªæ„
    search_query = text_content[:1000]

    # =========================================================
    # æŸ¥è©¢ cti-knowledge-base (å¤–éƒ¨çŸ¥è­˜)
    # ç›®çš„ï¼šæ‰¾å‡ºé€™æ®µæ–‡å­—æ¶‰åŠä»€éº¼ã€Œæ”»æ“Šæ‰‹æ³•ã€æˆ–ã€Œé˜²ç¦¦å»ºè­°ã€
    # =========================================================
    try:
        kb_response = client.search(
            index="cti-knowledge-base",
            body={
                "size": top_k,
                "query": {
                    "multi_match": {
                        "query": search_query,
                        # æ¨™é¡Œ (name) æ¬Šé‡ x3ï¼Œæè¿° (description) æ¬Šé‡ x1
                        "fields": ["name^3", "description"], 
                        "type": "best_fields"
                    }
                }
            }
        )
        
        hits = kb_response.get("hits", {}).get("hits", [])
        if hits:
            attacks = []
            defenses = []
            
            for hit in hits:
                src = hit["_source"]
                # æ ¼å¼ï¼š[ä¾†æº] ID åç¨±: æè¿°ç‰‡æ®µ
                info = f"- [{src.get('source', 'UNK').upper()}] {src.get('name', '')} ({src.get('external_id', 'N/A')}): {src.get('description', '')[:200]}..."
                
                if src.get("type") == "defense":
                    defenses.append(info)
                else:
                    attacks.append(info)
            
            # çµ„è£çŸ¥è­˜åº« Context
            if attacks:
                context_parts.append("  [Known Attack Patterns (MITRE ATT&CK)]:")
                context_parts.extend(attacks)
            if defenses:
                context_parts.append("  [Recommended Defenses (AI Defense)]:")
                context_parts.extend(defenses)
                
    except Exception as e:
        logger.warning(f"  Knowledge Base search failed: {e}")

    # =========================================================
    # æŸ¥è©¢ cti-reports (å…§éƒ¨æ­·å²)
    # ç›®çš„ï¼šæ‰¾å‡ºã€Œä»¥å‰é‡éé€™ä»¶äº‹å—ï¼Ÿã€
    # =========================================================
    try:
        report_response = client.search(
            index="cti-reports",
            body={
                "size": top_k,
                "query": {
                    "more_like_this": {
                        # åœ¨é€™äº›æ¬„ä½ä¸­å°‹æ‰¾ç›¸ä¼¼å…§å®¹
                        "fields": ["content", "analysis_summary", "log_text"],
                        "like": search_query,
                        "min_term_freq": 1,
                        "max_query_terms": 12
                    }
                }
            }
        )
        
        hits = report_response.get("hits", {}).get("hits", [])
        if hits:
            history = []
            for hit in hits:
                src = hit["_source"]
                # æ’é™¤è‡ªå·± (å¦‚æœæ˜¯é‡è·‘çš„è©±)
                if src.get("content") == text_content: continue
                
                # æ ¼å¼ï¼šæ—¥æœŸ | æª”å | æ‘˜è¦
                info = f"- {src.get('timestamp', 'Unknown Date')[:10]} | {src.get('filename', 'Unknown')}: {src.get('analysis_summary', 'No summary')[:150]}..."
                history.append(info)
            
            if history:
                context_parts.append("\n  [Similar Past Internal Incidents]:")
                context_parts.extend(history)

    except Exception as e:
        logger.warning(f"  Historical Report search failed: {e}")

    # =========================================================
    # å›å‚³æœ€çµ‚çµ„åˆæ–‡å­—
    # =========================================================
    if not context_parts:
        return ""
        
    return "\n".join(context_parts)


# ================= ğŸ§¹ è‡ªå‹•æ¸…ç†èˆŠæª”æ¡ˆé‚è¼¯ =================
def cleanup_old_files():
    """
    å®šæœŸä»»å‹™ï¼šåˆªé™¤ processed è³‡æ–™å¤¾ä¸­è¶…é RETENTION_DAYS çš„èˆŠæª”æ¡ˆ
    """
    RETENTION_DAYS = 30  #  è¨­å®šä¿ç•™å¹¾å¤©
    cutoff_time = time.time() - (RETENTION_DAYS * 86400) # è¨ˆç®—æˆªæ­¢æ™‚é–“æˆ³
    
    logger.info(f"ğŸ§¹ [Cleanup] Starting cleanup of files older than {RETENTION_DAYS} days...")
    
    deleted_count = 0
    try:
        # æƒæ processed è³‡æ–™å¤¾
        for filename in os.listdir(PROCESSED_DIR):
            file_path = os.path.join(PROCESSED_DIR, filename)
            
            # åªè™•ç†æª”æ¡ˆï¼Œä¸è™•ç†è³‡æ–™å¤¾
            if os.path.isfile(file_path):
                file_mtime = os.path.getmtime(file_path)
                
                # å¦‚æœæª”æ¡ˆä¿®æ”¹æ™‚é–“æ—©æ–¼æˆªæ­¢æ™‚é–“ï¼Œå°±åˆªé™¤
                if file_mtime < cutoff_time:
                    try:
                        os.remove(file_path)
                        deleted_count += 1
                    except OSError as e:
                        logger.error(f"  Failed to delete {filename}: {e}")
                        
        if deleted_count > 0:
            logger.info(f"  [Cleanup] Deleted {deleted_count} old files.")
        else:
            logger.info("  [Cleanup] No files needed deletion.")
            
    except Exception as e:
        logger.error(f"  [Cleanup] Error during cleanup: {e}")

# ================= è‡ªå‹•æ›´æ–°æ’ç¨‹ =================

def update_knowledge_base_job():
    """
    å®šæœŸä»»å‹™ï¼šåŸ·è¡Œ setup_knowledge_base.py ä»¥æ›´æ–° MITRE / AI Defense è³‡æ–™
    """
    logger.info("  [CronJob] Starting scheduled Knowledge Base update (AI Defense/MITRE)...")
    try:
        # å‘¼å«å¦å¤–ä¸€æ”¯ Python è…³æœ¬ä¾†åŸ·è¡Œä¸‹è¼‰èˆ‡æ›´æ–°
        result = subprocess.run(
            ["python", "/app/src/setup_knowledge_base.py"], 
            capture_output=True, 
            text=True
        )
        
        if result.returncode == 0:
            logger.info("  [CronJob] Knowledge Base updated successfully.")
            # logger.info(result.stdout) # å¦‚æœè¦çœ‹è©³ç´°è¼¸å‡ºå¯ä»¥å–æ¶ˆè¨»è§£
        else:
            logger.error(f"  [CronJob] Update script failed:\n{result.stderr}")
            
    except Exception as e:
        logger.error(f"  [CronJob] Execution error: {e}")

def run_scheduler_thread():
    """èƒŒæ™¯æ’ç¨‹åŸ·è¡Œç·’"""
    logger.info("  Scheduler initialized. AI Defense will update periodically.")
    
    # è¨­å®šæ’ç¨‹ï¼šé è¨­ æ¯ 12 å°æ™‚æ›´æ–°ä¸€æ¬¡
    schedule.every(12).hours.do(update_knowledge_base_job)
    # æ¯å¤©å‡Œæ™¨ 03:00 åŸ·è¡Œç¡¬ç¢Ÿå¤§æƒé™¤
    schedule.every().day.at("03:00").do(cleanup_old_files)

    # å•Ÿå‹•æ™‚å…ˆç«‹å³è·‘ä¸€æ¬¡ï¼Œç¢ºä¿è³‡æ–™åº«æœ‰æœ€æ–°è³‡æ–™
    update_knowledge_base_job()

    while True:
        schedule.run_pending()
        time.sleep(60) # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰ä»»å‹™è¦è·‘

# ============================================================

def move_to_processed(file_path, filename):
    """å°‡è™•ç†å®Œçš„æª”æ¡ˆç§»è‡³ processed è³‡æ–™å¤¾"""
    # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜ï¼Œé¿å…æª”åé‡è¤‡
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    processed_dest = os.path.join(PROCESSED_DIR, f"{timestamp}_{filename}")
    
    try:
        if os.path.exists(file_path):
            os.rename(file_path, processed_dest)
            logger.info(f"  File archived to: {processed_dest}")
        else:
            logger.warning(f"  File not found during archive: {file_path}")
    except OSError as e:
        logger.error(f"  Error archiving file: {e}")


def process_task(task_payload: dict, llm: LLMClient):
    """
    Worker é‚è¼¯ï¼šæ”¯æ´ã€Œæª”æ¡ˆæ¨¡å¼ã€èˆ‡ã€Œä¸²æµæ¨¡å¼ã€çš„é€šç”¨è™•ç†å™¨
    Payload æ ¼å¼ç¯„ä¾‹ï¼š
      1. æª”æ¡ˆ: {"file_path": "/app/data/input/LOG.txt", "filename": "LOG.txt"}
      2. ä¸²æµ: {"timestamp": "...", "source_ip": "...", "message": "..."}
    """
    
    # --- åˆ¤æ–·ä¾†æºä¸¦å–å¾—å…§å®¹ ---
    file_path = None
    
    if "file_path" in task_payload:
        # [æ–¹æ³• A: æª”æ¡ˆè™•ç†] Master çš„ä»»å‹™
        filename = task_payload.get("filename", "unknown.txt")
        file_path = task_payload["file_path"]
        logger.info(f"  [File Mode] Processing: {filename}")
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                raw_content = f.read()
        except Exception as e:
            logger.error(f"  Failed to read file: {e}")
            return
            
    else:
        # [æ–¹æ³• B: ä¸²æµè™•ç†] Fluent Bit çš„å³æ™‚ Log
        logger.info(f"âš¡ [Stream Mode] Processing live log event")
        
        # è‡ªå‹•ç”Ÿæˆä¸€å€‹è™›æ“¬æª”åï¼Œæ–¹ä¾¿å¾ŒçºŒé‚è¼¯è­˜åˆ¥
        # æ ¼å¼: LOG_STREAM_{timestamp}_{source_ip}
        ts = int(time.time())
        src_ip = task_payload.get('source_ip', 'unknown_ip')
        filename = f"LOG_STREAM_{ts}_{src_ip}"
        
        # å°‡ JSON ç‰©ä»¶è½‰å›å­—ä¸²ï¼Œå› ç‚ºå¾Œé¢çš„ PII Masker å’Œåˆ‡ç‰‡å™¨åƒçš„æ˜¯å­—ä¸²
        raw_content = json.dumps(task_payload, ensure_ascii=False)

    # ================= PII é®ç½© (éš±ç§ä¿è­·) =================
    masker = PIIMasker()
    safe_content = masker.mask(raw_content) 
    
    if safe_content != raw_content:
        logger.info("  PII Masking applied (Emails/Phones redacted).")

    is_log = filename.startswith("LOG_")
    normalized_data = {} # å­˜æ”¾çµæ§‹åŒ–è³‡æ–™

    # ================ AI è‡ªé©æ‡‰æ­£è¦åŒ–  =================
    if is_log:
        try:
            # å˜—è©¦ç›´æ¥è§£æ (å¦‚æœ Fluent Bit å·²ç¶“é€ä¾† JSONï¼Œé€™è£¡æœƒæˆåŠŸ)
            normalized_data = json.loads(safe_content)
            logger.info(f"  {filename} is valid JSON. Using natively.")
        except json.JSONDecodeError:
            # å¦‚æœæ˜¯ Raw Textï¼Œå‘¼å« AI é€²è¡Œæ­£è¦åŒ–
            logger.info(f"  {filename} is raw text. Starting AI Adaptive Normalization...")
            normalized_data = llm.normalize_log(safe_content)
            
            if not normalized_data:
                normalized_data = {}
            
            # æ›´æ–° safe_content ç‚ºæ¨™æº–åŒ–å¾Œçš„ JSON å­—ä¸²
            safe_content = json.dumps(normalized_data, ensure_ascii=False)
            logger.info(f"  AI Normalized Data: {safe_content}")

    # ================= é•·æ–‡æœ¬åˆ‡ç‰‡èˆ‡ AI åˆ†æ =================
    chunks = chunk_text(safe_content)
    if len(chunks) > 1:
        logger.info(f"  Large document detected! Split into {len(chunks)} chunks.")

    chunk_results = []
    has_rag_context = False

    for i, chunk in enumerate(chunks):
        # RAG: åŒæ™‚æŸ¥è©¢çŸ¥è­˜åº«èˆ‡æ­·å²å ±å‘Š
        rag_context = retrieve_context(chunk)
        if rag_context:
            has_rag_context = True

        enhanced_chunk = f"{rag_context}\n\n[Report Segment {i+1}]:\n{chunk}"

        try:
            chunk_extraction = llm.get_extraction(enhanced_chunk)
            if chunk_extraction:
                chunk_results.append(chunk_extraction)
        except Exception as e:
            logger.error(f"  Error analyzing chunk {i+1}: {e}")
            continue

    # ================= åˆä½µåˆ†æçµæœ =================
    if not chunk_results:
        logger.warning("  LLM returned empty result for all chunks.")
        if file_path: move_to_processed(file_path, filename)
        return

    extracted = merge_extractions(chunk_results)

    # åˆä½µæ­£è¦åŒ–è³‡æ–™ (è£œé½Š IP ç­‰æ¬„ä½)
    if is_log and normalized_data:
        if "source_ip" in normalized_data:
            if "indicators" not in extracted: extracted["indicators"] = {}
            if "ipv4" not in extracted["indicators"]: extracted["indicators"]["ipv4"] = []
            
            ip = normalized_data["source_ip"]
            if ip and ip not in extracted["indicators"]["ipv4"]:
                extracted["indicators"]["ipv4"].append(ip)
        
        extracted.update(normalized_data)

    logger.info(f"  Analysis Complete. Merged {len(chunk_results)} chunks.")

    # çµ±ä¸€æ¬„ä½åç¨±
    if "confidence" not in extracted and "confidence_score" in extracted:
        extracted["confidence"] = extracted["confidence_score"]

    # é—œéµå­—å¼·åˆ¶è£œåˆ†
    current_score = extracted.get("confidence", 0)
    critical_keywords = [
        "CRITICAL", "RANSOMWARE", "MALWARE", "ATTACK", "BLOCKED", "CISA", 
        "JNDI", "EXPLOIT", "UNAUTHORIZED", "DENIED", "FAILED LOGIN", "ROOT", "SQL INJECTION"
    ]    
    if current_score < 50:
        content_upper = safe_content.upper()
        if any(k in content_upper for k in critical_keywords):
            logger.warning(f"  LLM gave low score ({current_score}), but found CRITICAL keywords. Overriding to 95!")
            extracted["confidence"] = 95
    
    if extracted.get("confidence") is None:
        extracted["confidence"] = 0

    # ================= è±å¯ŒåŒ– (Enrichment) =================
    try:
        enricher = EnrichmentEngine()
        enriched_data = {}
        indicators = extracted.get("indicators", {})
        
        for ip in indicators.get("ipv4", []):
            info = enricher.enrich_ip(ip)
            if info["geo"] or info["asset"]:
                enriched_data[ip] = info
        
        extracted["enrichment"] = enriched_data
        enricher.close()
        
    except Exception as e:
        logger.error(f"  Enrichment failed: {e}")

    # ================= 1.6 CVE æ¼æ´é—œè¯ =================
    try:
        cve_enricher = CVEEnricher()
        vulnerability_results = []

        ai_cve_ids = extracted.get("cve_ids", [])
        regex_cve_ids = cve_enricher.extract_cve_ids(raw_content)
        all_cve_ids = list(set(ai_cve_ids + regex_cve_ids))
        
        if all_cve_ids:
            logger.info(f"  Identifed CVEs (AI + Regex): {all_cve_ids}")

        for cid in all_cve_ids:
            if not any(v["id"] == cid for v in vulnerability_results):
                details = cve_enricher.get_cve_details(cid)
                if details:
                    vulnerability_results.append(details)

        extracted["vulnerabilities"] = vulnerability_results
        
        if any(isinstance(v.get("score"), (int, float)) and v.get("score") >= 7.0 for v in vulnerability_results):
            if extracted.get("confidence", 0) < 90:
                extracted["confidence"] = 95
                logger.warning("  Critical Vulnerability Detected! Confidence boosted to 95.")

    except Exception as e:
        logger.error(f"  CVE Enrichment Failed: {e}")

    # ================= å»ºç«‹ STIX Bundle =================
    stix_bundle = build_stix_bundle(extracted)
    stix_json_str = json.dumps(stix_bundle, indent=4, ensure_ascii=False)
        
    static_out_path = os.path.join(OUTPUT_DIR, "bundle_stix21.json")
    with open(static_out_path, "w", encoding="utf-8") as f:
        f.write(stix_json_str)

    # ================= æ™ºæ…§åˆ†æµè·¯ç”±é‚è¼¯ ===================
    is_log = filename.startswith("LOG_")               # ç¢ºä¿ is_log å®šç¾©æ¸…æ™°
    is_rss = filename.startswith("RSS_")
    is_otx = filename.startswith("OTX_CTI_")           # è¾¨è­˜ OTX ä¾†æº
    is_abusech = filename.startswith("ABUSECH_CTI_")   # è¾¨è­˜ Abuse.ch ä¾†æº
    is_github = filename.startswith("GITHUB_CTI_")     # è¾¨è­˜ GitHub ä¾†æº
    is_premium_feed = is_otx or is_abusech or is_github # çµ±ç¨±ç‚ºå°ˆæ¥­å¨è„…æƒ…è³‡

    confidence = extracted.get("confidence", 0)
    threat_matched = has_rag_context 

    # å®šç¾©ã€Œæœ‰å¯¦è³ªæŒ‡æ¨™ã€çš„æƒ…å ±ï¼šåŒ…å« ipv4, domains, urls, cve_ids
    has_indicators = bool(extracted.get("indicators", {}).get("ipv4") or 
                          extracted.get("indicators", {}).get("domains") or 
                          extracted.get("indicators", {}).get("urls") or 
                          extracted.get("cve_ids"))

    # å®šç¾©è‡ªå‹•é€šé—œæ¢ä»¶ (Auto-Pilot Condition)
    auto_pilot_condition = (
        is_rss or 
        (is_log and (confidence >= 80 or threat_matched)) or
        (is_premium_feed and confidence >= 70 and has_indicators)
    )

    # 1. è‡ªå‹•åŒ–é€šé“ (Auto-Pilot)
    if auto_pilot_condition:
        
        # ä¾æ“šä¸åŒä¾†æºçµ¦äºˆä¸åŒçš„è§¸ç™¼åŸå› æ—¥èªŒ
        if is_rss:
            trigger_reason = "RSS Feed"
        elif is_otx:
            trigger_reason = f"AlienVault OTX Pulse ({confidence}%)"
        elif is_abusech:
            trigger_reason = f"Abuse.ch Malware IoC ({confidence}%)"
        elif is_github:
            trigger_reason = f"GitHub Security Advisory ({confidence}%)"
        else:
            trigger_reason = f"High Confidence Log ({confidence}%)"
            
        logger.info(f"  Automated Pipeline Triggered [{trigger_reason}]. Blocking & Reporting...")

        pdf_filename = f"{os.path.splitext(filename)[0]}.pdf"
        pdf_path = os.path.join("data/reports", pdf_filename)
        generate_pdf_report(extracted, pdf_path)
        
        indicators = extracted.get("indicators", {})
        report_info = {"filename": filename, "confidence": confidence}
        
        for ip in indicators.get("ipv4", []):
            upsert_indicator(ip, "ipv4", report_info)
        for domain in indicators.get("domains", []):
            upsert_indicator(domain, "domain", report_info)

        doc = extracted.copy()
        
        # ä¾æ“šä¾†æºè¨­å®šè³‡æ–™åº«ä¸­çš„ source_type æ¨™ç±¤
        if is_rss:
            src_type = "rss"
        elif is_premium_feed:
            src_type = "premium_cti_feed"
        else:
            src_type = "log_automation"

        doc.update({
            "filename": filename,
            "timestamp": datetime.now().isoformat(),
            "expiration_date": (datetime.now() + timedelta(days=30)).isoformat(),
            "pdf_path": pdf_path,
            "source_type": src_type,
            "threat_matched": threat_matched
        })
        
        report_id = os.path.splitext(filename)[0] 
        upload_to_os_lib(doc, report_id, "cti-reports")

        # åŒæ­¥å¯«å…¥ SOC Dashboard (åªè™•ç† is_log)
        if is_log:
            soc_doc = doc.copy()
            first_ip = indicators.get("ipv4", [None])[0]
            soc_doc["source_ip"] = first_ip if first_ip else "Unknown"
            soc_doc["log_text"] = raw_content
            soc_doc["message"] = raw_content
            soc_doc["threat_matched"] = True 
            soc_doc["attack_type"] = extracted.get("attack_type", "High Confidence Threat")
            soc_doc["severity"] = "Critical" if confidence >= 90 else "High"
            soc_doc["mitigation_status"] = "Blocked  "
            
            upload_to_os_lib(soc_doc, f"log_{report_id}", "security-logs-knn")
            logger.info(f"  Synced to SOC Dashboard (security-logs-knn)")

    # 2. å™ªéŸ³éæ¿¾é€šé“ (ç›´æ¥ä¸Ÿæ£„/ä¸è™•ç†)
    elif (is_log and confidence < 40) or (is_premium_feed and confidence < 50):
        # å¦‚æœæƒ…å ±è§£æå¤±æ•—æˆ–è¢« AI åˆ¤å®šç‚ºç„¡ç”¨é›œè¨Šï¼Œå‰‡éæ¿¾æ‰
        logger.info(f"  Low confidence data ({confidence}%). Archiving without review.")

    # 3. äººå·¥å¯©æ ¸é€šé“ (Human-in-the-Loop)
    else:
        # é‚Šç·£æ¡ˆä¾‹ï¼Œä¾‹å¦‚æœ‰æƒ…å ±ä½† AI ä¸å¤ªç¢ºå®šï¼Œä¸Ÿçµ¦äººå·¥å¯©æ ¸
        if is_log:
            source_type = "suspicious_log"
            log_msg = f"ğŸ¤” Suspicious Log ({confidence}%). Sending to Human Review..."
        elif is_premium_feed:
            source_type = "unverified_cti_feed"
            log_msg = f"ğŸ¤” Unverified Premium CTI ({confidence}%). Sending to Human Review..."
        else:
            source_type = "manual"
            log_msg = f"ğŸ¤” Ambiguous Manual Upload ({confidence}%). Sending to Human Review..."
            
        logger.info(log_msg)
        
        insert_task(
            filename=filename,
            source_type=source_type,
            raw_content=safe_content,
            analysis_json=extracted,
            confidence=confidence
        )
    # ================= å°‡åˆ†æçµæœå­˜ç‚º JSON å‚™ä»½ =================
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # å¦‚æœæ˜¯ä¸²æµæ¨¡å¼ï¼Œfilename å·²ç¶“åŒ…å«æ™‚é–“æˆ³ï¼Œé€™è£¡å†åŠ ä¸€æ¬¡å‰ç¶´æˆ–æ˜¯å¯ä»¥ç°¡åŒ–
        json_filename = f"{timestamp}_{os.path.splitext(filename)[0]}.json"
        json_path = os.path.join(PROCESSED_DIR, json_filename)
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(extracted, f, indent=4, ensure_ascii=False)
            
        logger.info(f"  Analysis JSON saved to: {json_path}")
    except Exception as e:
        logger.error(f"  Failed to save JSON file: {e}")

    # ================= æ­¸æª” (åªé‡å°æª”æ¡ˆæ¨¡å¼) =================
    if file_path:
        move_to_processed(file_path, filename)


# --- Master: ç›£æ§æª”æ¡ˆä¸¦ç™¼é€ä»»å‹™ ---
def run_master():
    logger.info("  CTI Master Started. Monitoring data/input/ ...")
    
    # å•Ÿå‹•èƒŒæ™¯æ›´æ–°åŸ·è¡Œç·’ (æ’ç¨‹å™¨)
    updater_thread = threading.Thread(target=run_scheduler_thread, daemon=True)
    updater_thread.start()
    
    while True: # ç¬¬ä¸€å±¤ï¼šé€£ç·šé‡è©¦
        connection = None
        try:
            connection, channel = get_rabbitmq_connection()
            
            # å®£å‘Š Exchange å’Œ Queue æ‹“æ¨¸
            # ç‚ºäº†ç¢ºä¿ Master ç™¼å‡ºå»çš„ä»»å‹™ï¼Œè·Ÿ Fluent Bit ç™¼å‡ºå»çš„ Logï¼Œéƒ½èµ°åŒä¸€æ¢è·¯
            channel.exchange_declare(exchange='cti_exchange', exchange_type='direct')
            channel.queue_declare(queue=RABBITMQ_QUEUE, durable=True)
            channel.queue_bind(exchange='cti_exchange', queue=RABBITMQ_QUEUE, routing_key='cti_queue')
            
            logger.info("  Master connected to RabbitMQ (Binding: cti_exchange -> cti_queue).")

            while True: # ç¬¬äºŒå±¤ï¼šç›£æ§è¿´åœˆ
                # æª¢æŸ¥é€£ç·š
                if connection.is_closed:
                    raise pika.exceptions.AMQPConnectionError("Connection closed")

                # æƒæ txt æª”æ¡ˆ
                files = glob.glob(os.path.join(INPUT_DIR, "*.txt"))
                for file_path in files:
                    filename = os.path.basename(file_path)
                    
                    # è·³éå·²ç¶“è¢«é–å®šçš„æª”æ¡ˆ
                    if filename.endswith(".processing"):
                        continue

                    logger.info(f"ğŸ“‚ Found new file: {filename}")
                    
                    # é–å®šæª”æ¡ˆ (æ”¹å)
                    processing_path = file_path + ".processing"
                    try:
                        os.rename(file_path, processing_path)
                    except OSError:
                        continue # å¯èƒ½è¢«å…¶ä»– process æ¶èµ°äº†

                    # ç™¼é€ä»»å‹™
                    task_payload = json.dumps({
                        "file_path": processing_path,
                        "filename": filename
                    })
                    
                    try:
                        # ç™¼é€è‡³æŒ‡å®šçš„ Exchange å’Œ Routing Key
                        channel.basic_publish(
                            exchange='cti_exchange',  # æŒ‡å®šäº¤æ›æ©Ÿ
                            routing_key='cti_queue',  # æŒ‡å®šè·¯ç”±éµ (è·Ÿ Fluent Bit ä¸€æ¨£)
                            body=task_payload,
                            properties=pika.BasicProperties(delivery_mode=2) 
                        )
                        logger.info(f"  Task queued: {filename}")
                        
                    except (pika.exceptions.AMQPError, pika.exceptions.StreamLostError) as e:
                        # é€™æ˜¯é€£ç·šå•é¡Œï¼Œéœ€è¦è®“å¤–å±¤è¿´åœˆé‡é€£
                        logger.error(f"  RabbitMQ Error during publish: {e}")
                        os.rename(processing_path, file_path) # å¾©åŸæª”æ¡ˆ
                        raise e # æ‹‹å‡ºéŒ¯èª¤è®“å¤–å±¤ while True é‡é€£
                    except Exception as e:
                        # é€™æ˜¯ç¨‹å¼é‚è¼¯æˆ–å…¶ä»–å•é¡Œ (ä¾‹å¦‚ JSON éŒ¯èª¤)ï¼Œä¸è¦è®“ Master å´©æ½°é‡å•Ÿ
                        logger.error(f"  Logical Error during publish: {e}")
                        os.rename(processing_path, file_path)
                        # ä¸è¦ raiseï¼Œè®“å®ƒç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹æª”æ¡ˆï¼Œé¿å…å¡æ­»

                time.sleep(2) # è¼ªè©¢é–“éš”

        except (pika.exceptions.AMQPError, pika.exceptions.AMQPConnectionError) as e:
            logger.error(f"  Master connection lost: {e}. Retrying in 5s...")
            time.sleep(5)
        except Exception as e:
            logger.error(f"  Master unexpected error: {e}. Restarting in 10s...")
            time.sleep(10)

# --- Worker: å¾ Queue é ˜ä»»å‹™ ---
def run_worker():
    logger.info("  CTI Worker Started. Waiting for tasks...")
    llm = LLMClient()
    
    while True:
        try:
            connection, channel = get_rabbitmq_connection()
            
            # å®£å‘Š Exchange èˆ‡ Queue ç¶å®š (è·Ÿ Master/Fluent Bit ä¸€è‡´)
            channel.exchange_declare(exchange='cti_exchange', exchange_type='direct')
            channel.queue_declare(queue=RABBITMQ_QUEUE, durable=True)
            channel.queue_bind(exchange='cti_exchange', queue=RABBITMQ_QUEUE, routing_key='cti_queue')
            
            channel.basic_qos(prefetch_count=1)
            logger.info("  Worker connected & listening on 'cti_queue'...")

            def callback(ch, method, properties, body):
                try:
                    # è§£æè¨Šæ¯ (å¯èƒ½æ˜¯ Master çš„æª”æ¡ˆè·¯å¾‘ï¼Œä¹Ÿå¯èƒ½æ˜¯ Fluent Bit çš„ JSON)
                    task_payload = json.loads(body)
                    
                    # å‘¼å«é€šç”¨çš„è™•ç†å‡½å¼
                    process_task(task_payload, llm)
                    
                    # ä»»å‹™æˆåŠŸï¼Œç™¼é€ ACK
                    ch.basic_ack(delivery_tag=method.delivery_tag)
                    
                except Exception as e:
                    logger.error(f"  Worker task error: {e}")
                    # é¿å…æ¯’è—¥è¨Šæ¯å¡æ­»ï¼Œé¸æ“‡ ACK ä¸¦è¨˜éŒ„éŒ¯èª¤ (ä¹Ÿå¯ä»¥ä¸ ACK ç›´æ¥é‡è©¦)
                    ch.basic_ack(delivery_tag=method.delivery_tag)

            channel.basic_consume(queue=RABBITMQ_QUEUE, on_message_callback=callback)
            channel.start_consuming()

        except Exception as e:
            logger.error(f"  Worker error: {e}. Restarting in 5s...")
            time.sleep(5)

if __name__ == "__main__":
    ensure_dirs()
    if ROLE == 'master':
        run_master()
    else:
        run_worker()