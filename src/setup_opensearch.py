import os
import logging
import hashlib
from datetime import datetime, timedelta
from dotenv import load_dotenv
from opensearchpy import OpenSearch, RequestsHttpConnection


load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OpenSearch é€£ç·šè¨­å®š
OPENSEARCH_HOST = os.getenv("OPENSEARCH_HOST", "opensearch-node")
OPENSEARCH_PORT = int(os.getenv("OPENSEARCH_PORT", 9200))
OPENSEARCH_USER = os.getenv("OPENSEARCH_USER", "admin")
OPENSEARCH_PASSWORD = os.getenv("OPENSEARCH_PASSWORD", "Admin123!")

# å®šç¾© Index åç¨±
INDEX_LOGS_KNN = "security-logs-knn"
INDEX_CTI_REPORTS = "cti-reports"
INDEX_CTI_INDICATORS = "cti-indicators"
INDEX_ALERTS = "security-alerts"
INDEX_AUDIT_LOGS = "soc-audit-logs"

def get_opensearch_client():
    host = os.getenv("OPENSEARCH_HOST", "opensearch-node")
    port = int(os.getenv("OPENSEARCH_PORT", 9200))
    user = os.getenv("OPENSEARCH_USER", "admin")
    password = os.getenv("OPENSEARCH_PASSWORD", "admin")

    connection_kwargs = {
        "hosts": [{'host': host, 'port': port}],
        "http_compress": True,
        "use_ssl": False,
        "verify_certs": False,
        "ssl_show_warn": False,
        "connection_class": RequestsHttpConnection
    }

    if user and password:
        connection_kwargs["http_auth"] = (user, password)

    return OpenSearch(**connection_kwargs)

def create_index():
    """ä¸»å‡½å¼ï¼šå»ºç«‹æ‰€æœ‰éœ€è¦çš„ Indexes"""
    client = get_opensearch_client()
    create_knn_index(client)
    create_reports_index(client)
    create_alerts_index(client)
    create_audit_index(client)
    apply_retention_policy(client)

def create_knn_index(client):
    """å»ºç«‹ Log å‘é‡æœå°‹ç”¨çš„ Index"""
    index_body = {
        "settings": {
            "index": {
                "knn": True,
                "knn.algo_param.ef_search": 100
            }
        },
        "mappings": {
            "properties": {
                "tenant_id": {"type": "keyword"},
                "timestamp": {"type": "date"},
                "log_text": {"type": "text"},
                "log_vector": {
                    "type": "knn_vector",
                    "dimension": 1536,
                    "method": {
                        "name": "hnsw",
                        "space_type": "cosinesimil",
                        "engine": "lucene",
                        "parameters": {"ef_construction": 128, "m": 24}
                    }
                },
                "log_source": {"type": "keyword"},
                "threat_matched": {"type": "boolean"},
                "confidence": {"type": "integer"},
                "source_type": {"type": "keyword"},
                "indicators": {"properties": {"ipv4": {"type": "keyword"}}}
            }
        }
    }
    _create_if_not_exists(client, INDEX_LOGS_KNN, index_body)

def create_reports_index(client):
    """å»ºç«‹å­˜æ”¾ CTI åˆ†æå ±å‘Šçš„ Index"""
    index_body = {
        "mappings": {
            "properties": {
                "tenant_id": {"type": "keyword"},
                "timestamp": {"type": "date"},
                "filename": {"type": "keyword"},
                "confidence": {"type": "integer"},
                "indicators": {"type": "object"},
                "ttps": {"type": "object"},
                "threat_matched": {"type": "boolean"},
                "source_type": {"type": "keyword"},
                "related_reports": {"type": "keyword"}
                
            }
        }
    }
    _create_if_not_exists(client, INDEX_CTI_REPORTS, index_body)

def create_alerts_index(client):
    """å»ºç«‹å­˜æ”¾è±å¯ŒåŒ–è­¦å ±çš„ Index"""
    index_body = {
        "mappings": {
            "properties": {
                "tenant_id": {"type": "keyword"},
                "timestamp": {"type": "date"},
                "rule_name": {"type": "keyword"},
                "asset_hostname": {"type": "keyword"},
                "asset_department": {"type": "keyword"},
                "asset_criticality": {"type": "keyword"},
                "status": {"type": "keyword"},
                "log_excerpt": {"type": "text"}
            }
        }
    }
    _create_if_not_exists(client, INDEX_ALERTS, index_body)

def create_audit_index(client):
    """å»ºç«‹å¯©è¨ˆæ—¥èªŒ Index (Immutable Log)"""
    index_body = {
        "mappings": {
            "properties": {
                "tenant_id": {"type": "keyword"},  # Multi-Tenancy Support
                "timestamp": {"type": "date"},
                "actor": {"type": "keyword"},
                "action": {"type": "keyword"},
                "target": {"type": "keyword"},
                "status": {"type": "keyword"},
                "justification": {"type": "text"},
                "event_id": {"type": "keyword"},
                "details": {"type": "object"}
            }
        }
    }
    _create_if_not_exists(client, INDEX_AUDIT_LOGS, index_body)

def update_mappings(client):
    """(Migrstion Utility) Update existing indices with new fields"""
    indices = [INDEX_LOGS_KNN, INDEX_CTI_REPORTS, INDEX_ALERTS, INDEX_AUDIT_LOGS]
    for idx in indices:
        if client.indices.exists(index=idx):
            try:
                client.indices.put_mapping(
                    index=idx,
                    body={"properties": {"tenant_id": {"type": "keyword"}}}
                )
                logger.info(f"  Updated mapping for {idx}: Added 'tenant_id'")
            except Exception as e:
                logger.warning(f"  Failed to update mapping for {idx}: {e}")

def apply_retention_policy(client):
    """
    æ¨¡æ“¬ ISM (Index State Management) Retention Policy
    åˆªé™¤è¶…é 180 å¤©çš„ Log ä»¥ç¬¦åˆéš±ç§æ³•è¦
    """
    indices_to_clean = [INDEX_LOGS_KNN, INDEX_CTI_REPORTS, INDEX_AUDIT_LOGS, INDEX_ALERTS]
    retention_days = 180
    
    # è¨ˆç®—æˆªæ­¢æ—¥æœŸ
    cutoff_date = (datetime.now() - timedelta(days=retention_days)).isoformat()
    
    logger.info(f"ğŸ§¹ [Retention Policy] Checking for data older than {retention_days} days (Cutoff: {cutoff_date})...")
    
    for idx in indices_to_clean:
        if not client.indices.exists(index=idx): continue
        
        query = {
            "query": {
                "range": {
                    "timestamp": {
                        "lt": cutoff_date
                    }
                }
            }
        }
        
        try:
            resp = client.delete_by_query(index=idx, body=query, refresh=True)
            deleted = resp.get('deleted', 0)
            if deleted > 0:
                logger.info(f"  [Retention] Deleted {deleted} old documents from index '{idx}'.")
        except Exception as e:
            logger.error(f"  [Retention] Failed to enact policy on '{idx}': {e}")

def _create_if_not_exists(client, index_name, body):
    if not client.indices.exists(index=index_name):
        try:
            client.indices.create(index=index_name, body=body)
            logger.info(f"  Index '{index_name}' created successfully.")
        except Exception as e:
            logger.error(f"  Failed to create index '{index_name}' : {e}")
    else:
        logger.info(f"   Index '{index_name}' already exists. Skipping.")

def upload_to_opensearch(doc_body, doc_id=None, index_name=INDEX_CTI_REPORTS):
    """é€šç”¨ä¸Šå‚³å‡½å¼"""
    client = get_opensearch_client()
    try:
        response = client.index(index=index_name, body=doc_body, id=doc_id, refresh=True)
        logger.info(f"  Upload success to {index_name} (ID: {response['_id']})")
        return True
    except Exception as e:
        logger.error(f"  Upload failed: {e}")
        return False

def upsert_indicator(indicator_value, indicator_type, report_data):
    """
    å»é‡é‚è¼¯ï¼šä½¿ç”¨ MD5 ä½œç‚ºæ–‡ä»¶ IDã€‚
    å¦‚æœæŒ‡æ¨™å·²å­˜åœ¨ï¼Œæ›´æ–° last_seen ä¸¦å»¶é•·éæœŸæ™‚é–“ï¼›è‹¥ä¸å­˜åœ¨å‰‡æ–°å¢ã€‚
    """
    client = get_opensearch_client()
    index_name = INDEX_CTI_INDICATORS
    
    # ç”¢ç”Ÿå”¯ä¸€ ID (å°å€¼é€²è¡Œé›œæ¹Š)
    doc_id = hashlib.md5(indicator_value.lower().encode()).hexdigest()
    
    # è¨ˆç®—æ–°çš„éæœŸæ™‚é–“ (å¾ç¾åœ¨èµ·ç®— 30 å¤©)
    new_expiry = (datetime.now() + timedelta(days=30)).isoformat()
    now_str = datetime.now().isoformat()
    
    # å®šç¾©æ›´æ–°è…³æœ¬
    update_body = {
        "script": {
            "source": """
                ctx._source.last_seen = params.now;
                ctx._source.expiration_date = params.expiry;
                if (!ctx._source.related_reports.contains(params.report)) {
                    ctx._source.related_reports.add(params.report);
                }
            """,
            "params": {
                "now": now_str,
                "expiry": new_expiry,
                "report": report_data.get("filename", "Unknown")
            }
        },
        "upsert": {
            "value": indicator_value,
            "type": indicator_type,
            "first_seen": now_str,
            "last_seen": now_str,
            "expiration_date": new_expiry,
            "related_reports": [report_data.get("filename", "Unknown")],
            "confidence": report_data.get("confidence", 50)
        }
    }
    
    try:
        client.update(index=index_name, id=doc_id, body=update_body, refresh=True)
        logger.info(f"  Indicator upserted: {indicator_value}")
    except Exception as e:
        # å¦‚æœç´¢å¼•æœªå»ºç«‹ï¼Œç›´æ¥å¯«å…¥ç¬¬ä¸€ç­†è³‡æ–™
        try:
            client.index(index=index_name, id=doc_id, body=update_body["upsert"], refresh=True)
            logger.info(f"  Indicator created (Fallback): {indicator_value}")
        except Exception as create_err:
             logger.error(f"  Failed to upsert indicator: {create_err}")

if __name__ == "__main__":
    create_index()
    try:
        client = get_opensearch_client()
        update_mappings(client)
    except: pass
    try:
        client = get_opensearch_client()
        update_mappings(client)
    except: pass