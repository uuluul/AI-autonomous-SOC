import os
import time
import schedule
import feedparser
import logging
import re
import hashlib
import json
from bs4 import BeautifulSoup
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - [RSS Crawler] - %(message)s')
logger = logging.getLogger(__name__)

# è³‡æ–™å¤¾èˆ‡æª”æ¡ˆè¨­å®š
INPUT_DIR = "data/input"
HISTORY_FILE = "data/rss_history.json" 

# RSS ä¾†æºæ¸…å–®
RSS_FEEDS = {
    "CISA_Alerts": "https://www.cisa.gov/uscert/ncas/alerts.xml",
    "TheHackerNews": "https://feeds.feedburner.com/TheHackersNews",
    "BleepingComputer": "https://www.bleepingcomputer.com/feed/"
}

def clean_filename(title):
    """å°‡æ¨™é¡Œè½‰ç‚ºåˆæ³•çš„æª”å"""
    clean = re.sub(r'[\\/*?:"<>|]', "", title)
    return clean.replace(" ", "_")[:100]

def clean_html(html_content):
    """ç§»é™¤ HTML æ¨™ç±¤ï¼Œåªä¿ç•™ç´”æ–‡å­—"""
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.get_text()

# === MD5 æ ¡é©—é‚è¼¯ ===

def get_content_hash(text):
    """å°æ–‡å­—å…§å®¹é€²è¡Œ MD5 ç·¨ç¢¼"""
    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()

def load_history():
    """è¼‰å…¥å·²æŠ“å–éçš„ Hash æ¸…å–®"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r") as f:
                return set(json.load(f))
        except:
            return set()
    return set()

def save_history(history_set):
    """å„²å­˜æœ€æ–°çš„ Hash æ¸…å–®"""
    with open(HISTORY_FILE, "w") as f:
        json.dump(list(history_set), f)

def fetch_rss():
    logger.info("ğŸ•·ï¸ Starting RSS fetch cycle...")
    
    # è¼‰å…¥æ­·å²ç´€éŒ„
    history = load_history()
    new_hashes_found = False
    
    for source_name, url in RSS_FEEDS.items():
        try:
            logger.info(f"Checking feed: {source_name}...")
            feed = feedparser.parse(url)
            
            for entry in feed.entries[:10]: # æ“´å¤§ç¯„åœï¼Œå¢åŠ å‘½ä¸­æ©Ÿæœƒ
                title = entry.title
                link = entry.link
                content_raw = entry.get('summary', entry.get('description', ''))
                content = clean_html(content_raw)
                published = entry.get('published', datetime.now().strftime('%Y-%m-%d'))
                
                # --- å…§å®¹æ ¡é©— ---
                # åªé‡å°å…§å®¹åš Hashï¼Œå¦‚æœå…§å®¹ä¸€æ¨¡ä¸€æ¨£ï¼Œå³ä¾¿æ¨™é¡Œè®Šäº†ä¹Ÿæœƒè¢«è·³é
                content_hash = get_content_hash(content)
                
                if content_hash in history:
                    logger.debug(f"  Skipping (Content Duplicate): {title[:30]}...")
                    continue

                # çµ„åˆæª”å
                safe_title = clean_filename(title)
                filename = f"RSS_{source_name}_{safe_title}.txt"
                filepath = os.path.join(INPUT_DIR, filename)
                
                # é¡å¤–æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ (é›™é‡ä¿éšª)
                processed_path = os.path.join("data/processed", filename)
                if os.path.exists(filepath) or os.path.exists(processed_path):
                    continue
                
                # å¯«å…¥æª”æ¡ˆ
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(f"Title: {title}\n")
                    f.write(f"Source: {source_name}\n")
                    f.write(f"Date: {published}\n")
                    f.write(f"Link: {link}\n")
                    f.write(f"Content_Hash: {content_hash}\n") # ç´€éŒ„åœ¨æª”æ¡ˆå…§æ–¹ä¾¿æº¯æº
                    f.write(f"\n[Content]\n{content}\n")
                
                # æ›´æ–°æ­·å²ç´€éŒ„
                history.add(content_hash)
                new_hashes_found = True
                logger.info(f"  New Report Fetched: {filename}")
                
        except Exception as e:
            logger.error(f"  Error fetching {source_name}: {e}")

    # åªæœ‰åœ¨æœ‰æ–°æ–‡ç« æ™‚æ‰æ›´æ–° history æª”æ¡ˆï¼Œæ¸›å°‘ IO è² æ“”
    if new_hashes_found:
        save_history(history)

def main():
    
    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    
    logger.info("  RSS Crawler Service Started with MD5 Deduplication.")
    logger.info(f"   Fingerprint database: {HISTORY_FILE}")
    
    fetch_rss()
    
    # è¨­å®šæ’ç¨‹
    schedule.every(30).minutes.do(fetch_rss)
    
    while True:
        schedule.run_pending()
        time.sleep(1)

if __name__ == "__main__":
    main()